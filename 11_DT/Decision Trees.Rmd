---
title: "Decision Trees"
author: "Brian Wright"
date: "November 27, 2017"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

Libraries
```{r}
library(rio)
library(plyr)
library(tidyverse)
library(rpart)
library(psych)
library(pROC)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("rattle")
library(rattle)
library(caret)
library(C50) #Need this to pass into caret 
library(mlbench)
```


Let's take a look at the caret method using C5.0, nice overview of C5.0 
and functions that build in the R version: https://www.rulequest.com/see5-unix.html

CARET Example using C5.0: Use a new Dataset multi-class and doing three data
partitions: Training, Tuning and Testing
```{r}

winequality <- read.csv("data/winequality-red-ddl.csv")
View(winequality)
#str(winequality)
table(winequality$text_rank)
# Kappa compares across multi-class 
winequality$text_rank <- fct_collapse(winequality$text_rank,
                                      ave=c("ave","average-ish"),
                                      excellent = "excellent",
                                      good = "good",
                                      poor = c("poor","poor-ish",""))
table(winequality$text_rank)
# f1 score is good for imbalanced mean: harmonic mean of precision and accuracy 
#what do you know about your target variable? What models might work best? what are my evaluation metrics 
# kNN and k-means are only numerical data 

```

Splitting the Data
```{r}
#There is not a easy way to create 3 partitions using the createDataPartitions
#so we are going to use it twice. Mostly because we want to stratify on the variable we are working to predict. What does that mean?  
part_index_1 <- caret::createDataPartition(winequality$text_rank,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)

train <- winequality[part_index_1, ]
tune_and_test <- winequality[-part_index_1, ]

#The we need to use the function again to create the tuning set 

tune_and_test_index <- createDataPartition(tune_and_test$text_rank,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]


dim(train)
dim(test)# these will be slightly off because the data set isn't perfectly even
#buts its not a issue. 
dim(tune)

```

if you use read.csv when you load it in, a . will be added into the variable name -- try to eliminate the space in a variable as much as you can 

```{r}
# Choose the features and classes

features <- train[,c(-12,-13)]#dropping 12 and 13. 12 essentially predicts 13 
#perfectly and 13 is our target variable
target <- train$text_rank

str(features)
str(target)

#Cross validation process 

fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5, 
                          returnResamp="all",
                          classProbs = TRUE,
                          allowParallel = TRUE) 
# standard approach to training machine learning models -- how do you want to train the model 
# number of splits -- 10 fold cross validation 
# returns the list of resamples that it did 
# builds the models in parallel using the processes on your computer -- not building the cross valdiation models independently but at the same time, using all your cores and making it more efficient 
# number - number of folds
# repeats - number of times the CV is repeated, here it's 5 take the average of
# those 5 repeats

# many ways that you might go through the training methods 
# just a superset of what could be many more 
# certain cross validation methods might be better -- could leave on out cross validation -- takes one column of data out and then trains each column -- maybe overfitting, would take a long time -- big dataset would not be appropriate to use leave one column out

# might want to try different cross validation methods 

# Grid search options for each of the models available in CARET
# http://topepo.github.io/caret/train-models-by-tag.html#tree-based-model

grid <- expand.grid(.winnow = c(TRUE,FALSE), 
                    .trials=c(1,5,10,15,20), 
                    .model="tree")

#expand.grid - series of options that are available for model training
#gridsearch: elements that are specific to the model that we're building that has ranges of the values that we're observing 
# model will pick the one that works the best based on the outcome of the grid 
#winnow - whether to reduce the feature space -  Works to remove unimportant -- goes through a process of eliminating variables based on their contribution to the model 
# if the error doesn't change, then it drops it and reduces the feature space
#trials: boosting trials, how many times it's going to build the model over and over again
# C5.0 is a boosting method that will select and will continue to include over the trials 
# combined with winnowing: the variables not working will get dropped 
#features but it doesn't always work, in the above we are winnowing.  

#Actually a pretty good StackExchange post on winnowing:
#https://stats.stackexchange.com/questions/83913/understanding-the-output-of-c5-0-classification-model-using-the-caret-package

#trails - number of boosting iterations to try, 1 indicates a single model 
#model - type of ml model

set.seed(1984)
wine_mdl <- train(x=features,
                y=target,
                method="C5.0",
                tuneGrid=grid,
                trControl=fitControl,
                verbose=TRUE)

wine_mdl #provides us the hyper-parameters that were selected through the grid
# search process. 

View(wine_mdl$pred)

# visualize the re-sample distributions
xyplot(wine_mdl,type = c("g", "p", "smooth"))
# shows an elbow plot the marginal gains show that the iterations should stop at 10. model complexity: just do 10 boosting iterations and then when we do the boostng then it can be changed to 10. 
varImp(wine_mdl)

```


Let's use the model to predict and the evaluate the performance
```{r}
# need to put the predictions in across the class 
wine_pred_tune = predict(wine_mdl,tune, type= "raw")

View(as_tibble(wine_pred_tune))


#Lets use the confusion matrix

(wine_eval <- confusionMatrix(as.factor(wine_pred_tune), 
                as.factor(tune$text_rank), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec"))

# predicting score is good 
# some inconsistency in the poor and some inconsistency in good but the problem associated is that we don't have a lot of excellent 
# basically randomly guessing for randomly guessing -- all in different categories 
# overall, the model works pretty well, but across the excellent category, not so much probably because we don't have enough examples.
table(tune$text_rank)

(wine_pred_tune_p = predict(wine_mdl,tune,type= "prob"))
# balanced -- looks like many of the variables are important
# fixed acidity would become the root node 
```
Next we need to use the "tune" dataset to optimize our model, then once 
we are confident that we've got the best model we see how it 
performs on the test set.

"C5.0 constructs decision trees in two phases. A large tree is first grown 
to fit the data closely and is then `pruned' by removing parts that are 
predicted to have a relatively high error rate (winnowing, as a example). 
This pruning process is first applied to every sub-tree to decide whether 
it should be replaced by a leaf or sub-branch, and then a global stage 
looks at the performance of the tree as a whole." - https://www.rulequest.com/see5-unix.html#SOFT

The process above is referred to as pessimistic pruning, as it's centered on 
whether a tree should be pruned or not. 
See page 381 Applied Predictive Modeling for more details
-- reusing the same functions that we've already used
-- not a default f1 score that we can optimize 
-- used an f1 function and used it for the summaryFunction in trainControl (can show the model that you want to optimize) 
-- changed the control method to leave group out cross validation and then the boosting rounds were expanded
-- continue to iterate on the tuned dataset and continue to predict with tune if the model is improving 
 -- probably not going to improve that much -- why we create the tuned dataset so that we can compare to the original model based on adjustments, training methods, and models 
-- expand grid: the boosting trials are the only variable that you can change 
-- there'll be constraints to a certain extent 
 
# Let's make some changes and see if we can improve

```{r}
#Cross Validation Process, changing method for CV and adding a different metric for optimization 

library(MLmetrics)
f1 <- function(data, lev = NULL, model = NULL) {
  f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
  c(F1 = f1_val)
}
#source: https://stackoverflow.com/questions/37666516/caret-package-custom-metric

fitControl_2 <- trainControl(method = "LGOCV",
                          number = 10, 
                          returnResamp="all",
                          classProbs = TRUE,
                          allowParallel = TRUE,
                          summaryFunction = f1) 

# grid search, increasing the boosting rounds 
grid_2 <- expand.grid(.winnow = c(TRUE,FALSE), 
                    .trials=c(10,15,20,25,30), 
                    .model="tree")

# training model
set.seed(1984)
wine_mdl_2 <- train(x=features,
                y=target,
                method="C5.0",
                tuneGrid=grid_2,
                metric="F1",
                trControl=fitControl_2)


wine_mdl
wine_mdl_2

# wine_mdl_rpart <- train(x=features,
#                 y=target,
#                 method="rpart",
#                 trControl=fitControl)
# rpart.plot(wine_mdl_2$finalModel)
# 
# wine_mdl_rpart

```

# Evaluation of model 2

```{r}
wine_pred_tune_2 = predict(wine_mdl_2,tune, type= "raw")


#Lets use the confusion matrix

(model_eval_2 <- confusionMatrix(as.factor(wine_pred_tune_2), 
                as.factor(tune$text_rank), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec"))
model_eval_2
wine_eval

```

# Now give the changes we made above let's final the model and check the metrics
# output on the test file. 

# Final Evaluation 
```{r}
wine_pred_test = predict(wine_mdl_2,test, type= "raw")

View(as_tibble(wine_pred_test))


#Lets use the confusion matrix

confusionMatrix(as.factor(wine_pred_test), 
                as.factor(test$text_rank), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec")


(wine_pred_tune_p = predict(wine_mdl,test,type= "prob"))
```

Example with rpart on a numerical dataset, which might work better with a rules
based approach. 


```{r}
tree_example <- tibble(import("data/pregnancy.csv", check.names= TRUE))

describe(tree_example)
View(tree_example)
str(tree_example)

#We want to build a classifier that can predict whether a shopper is pregnant based on the items they buy so we can direct-market to that customer if possible. 


sum(tree_example$PREGNANT)
length(tree_example$PREGNANT)

(x <- 1- sum(tree_example$PREGNANT)/length(tree_example$PREGNANT))


#What does .72 represent in this context? 

```



#reformat for exploration purposes
```{r}

#Creating a vertical dataframe for the pregnant variable, just stacking the variables on top of each other. 


tree_example_long = tree_example %>% gather(Var, #<- list of predictor variables
                                Value,#<- the values of those predictor variables
                                -PREGNANT)  #<- the column to gather the data by

View(tree_example)
View(tree_example_long)

```


#See what the base rate of likihood of pregnancy looks like for each variable
```{r}
# Calculate the probability of being pregnant by predictor variable.
# Since the data is binary you can take the average to get the probability.

#Older way, but works well for doing multi-level group summaries, creates new variables for each group versus a summary for the entire list. 



tree_example_long_form = ddply(tree_example_long, 
                            .(Var, Value),#<- group by Var and Value, "." allows us to call the variables without quoting
                            summarize,  
                            prob_pregnant = mean(PREGNANT), #<- probability of being pregnant
                            prob_not_pregnant = 1 - mean(PREGNANT)) #<- probability of not being pregnant

#?ddply

View(tree_example_long_form)
```

#Build  the model using rpart and CART Algo (Gini Index - Binary Tree)
```{r}
# In order for this decision tree algorithm to run, 
# all the variables will need to be turned into factors. 
#Make sure your variables are classified correctly. 


tree_example = lapply(tree_example, function(x) as.factor(x))

#This is a handy reference on apply(), lapply(), sapply() are 
#all essentially designed to avoid for loops, especially in combination 
#with (function (x))

#https://www.r-bloggers.com/using-apply-sapply-lapply-in-r/

str(tree_example)

tree_example <- as_tibble(tree_example)

table(tree_example$PREGNANT)

#Also want to add data labels to the target
tree_example$PREGNANT <- factor(tree_example$PREGNANT,labels = c("not_preg", "preg"))

#Build the model
# Train the tree with the rpart() function.
# We'll need to set the seed to make the results reproducible. 
set.seed(1980)
tree_example_tree_gini = rpart(PREGNANT~.,  #<- formula, response variable ~ predictors
                           #   "." means "use all other variables in data"
                            method = "class",#<- specify method, use "class" for tree
                            parms = list(split = "gini"),#<- method for choosing tree split
                            data = tree_example,#<- data used
                            control = rpart.control(cp=.001))

#Look at the results
tree_example_tree_gini

View(tree_example_tree_gini$frame)

# dev - the deviance or the total sum of squares within the node, so if
#       you divide this by the sample size in each node you get the variance
# yval - average value of the trait at the node (for categorical values identifies the group)  
# complexity - the value of the parameter used to make the split (gini or information gain)
# ncompete - number of competing variables that can be considered for this split
# nsurrogate - number of surrogate trees (used when there is missing data in the test data set, to mimic the effects of splits in the training data set)
# yval2 - average value of the trait at the node (for categorical values identifies the group), although it can mean different things when the rpart function is used for regression trees or other analyses 


rpart.plot(tree_example_tree_gini, type =4, extra = 101)#package rpart.plot
#export this to  pdf for better viewing
?rpart.plot

#The "cptable" element includes the optimal prunnings based on the complexity parameter.

View(tree_example_tree_gini$cptable)

plotcp(tree_example_tree_gini)#Produces a "elbow chart" for various cp values

# Here's a summary:
# CP - complexity parameter, or the value of the splitting criterion (gini or information gain)
# nsplit - number of splits
# rel error - the relative error rate for predictions for the data that generated the tree
# xerror - cross-validated error, default cross-validation setting uses 10 folds
# xstd - the standard derivation of cross-validated errors

# NOTE: 
# For pruning a tree, the rule of thumb is to choose the split at the lowest level 
# where the rel_error + xstd < xerror

cptable_ex <- as_tibble(tree_example_tree_gini$cptable, )
str(cptable_ex)

cptable_ex$opt <- cptable_ex$`rel error`+ cptable_ex$xstd

View(cptable_ex)

# Ok so let's compare the cptable_ex, the cpplot and the decision tree plot, 
# they all covered around 8ish splits of the tree or a cp of .014ish. 
# Print out skips splits that result in terminal leaf nodes for 
# some reason, so makes it a little hard to interpret 

rpart.plot(tree_example_tree_gini, type =4, extra = 101)

# Shows the reduction in error provided by including a given variable 
tree_example_tree_gini$variable.importance

```

#Plot the Output to png 

```{r}
# Plot tree, and save to a png file.
png("Pregnancy_tree_gini.png",  #<- image name
    width = 1000,               #<- width of image in pixels
    height = 600)               #<- height of image in pixels

post(tree_example_tree_gini,                  #<- the rpart model to plot
     file = "",                            #<- ensure the png file is created correctly
     title = "Tree for Pregnancy - gini")  #<- the title of the graph

dev.off()
```


# Test the accuracy 
```{r}
# Let's use the "predict" function to test our our model and then 
# evaluate the accuracy of the results.
tree_example_fitted_model = predict(tree_example_tree_gini, type= "class")

View(as.data.frame(tree_example_fitted_model))

#tree_example_fitted_model <- as.numeric(tree_example_fitted_model)
View(tree_example_fitted_model)

# Let's compare the results to the actual data.
preg_conf_matrix = table(tree_example_fitted_model, tree_example$PREGNANT)
preg_conf_matrix

table(tree_example_fitted_model)

#We can also just use the confusion matrix

library(caret)
confusionMatrix(as.factor(tree_example_fitted_model), 
                as.factor(tree_example$PREGNANT), 
                positive = "preg", 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec")

table(tree_example$PREGNANT)

```

Hit Rate or True Classification Rate, Detection Rate and ROC
```{r}
# The error rate is defined as a classification of "Pregnant" when 
# this is not the case, and vice versa. It's the sum of all the
# values where a column contains the opposite value of the row.
sum(preg_conf_matrix[row(preg_conf_matrix)!= col(preg_conf_matrix)])
# 301


# The error rate divides this figure by the total number of data points
# for which the forecast is created.
sum(preg_conf_matrix)
# 2000

# Let's use these values in 1 calculation.
preg_error_rate = sum(preg_conf_matrix[row(preg_conf_matrix) != col(preg_conf_matrix)])/ sum(preg_conf_matrix)


paste0("Hit Rate/True Error Rate:", preg_error_rate * 100, "%")
# "Hit Rate/True Error Rate:15.05%"


#Detection Rate is the rate at which the algo detects the positive class in proportion to the entire classification A/(A+B+C+D) where A is poss poss

preg_conf_matrix

preg_conf_matrix[2,2]/sum(preg_conf_matrix)# 17.75%, want this to be higher but only so high it can go, in a perfect model for this date it would be:


preg_roc <- roc(tree_example$PREGNANT, as.numeric(tree_example_fitted_model), plot = TRUE) #Building the evaluation ROC and AUV using the predicted and original target variables 

preg_roc

plot(preg_roc)

#We can adjust using a if else statement and the predicted prob

tree_example_fitted_prob = predict(tree_example_tree_gini, type= "prob")
View(tree_example_fitted_prob)

#Let's 
roc(tree_example$PREGNANT, ifelse(tree_example_fitted_prob[,'not_preg'] >= .25,0,1), plot=TRUE)

```

#We can also prune the tree to make it less complex 
```{r}
set.seed(1)
tree_example_tree_cp2 = rpart(PREGNANT~.,#<- formula, response variable ~ predictors,"." means "use all other variables in data"
                           method = "class", #<- specify method, use "class" for tree
                           parms = list(split = "gini"),#<- method for choosing tree split
                           data = tree_example,#<- data used
                           control = rpart.control(maxdepth = 7)) #<- includes depth zero, the control for additional options (could use CP, 0.01 is the default)

?rpart.control

plotcp(tree_example_tree_cp2)

View(tree_example_tree_cp2)

rpart.plot(tree_example_tree_cp2, type =4, extra = 101)

cptable_ex_cp <- as.data.frame(tree_example_tree_cp2$cptable, )
View(cptable_ex_cp)

cptable_ex_cp$opt <- cptable_ex_cp$`rel error`+ cptable_ex_cp$xstd

View(cptable_ex_cp)

#Change the rpart.control and take a look at results.

dev.off()

```
