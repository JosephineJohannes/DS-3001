---
title: "Random_Forest_Lab"
author: "Josephine Johannes"
date: "11/16/2020"
output: html_document
editor_options: 
chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(rio)
library(plyr)
library(tidyverse)
library(rpart)
library(psych)
library(pROC)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("rattle")
library(rattle)
library(caret)
library(C50) #Need this to pass into caret 
library(mlbench)
library(MLmetrics)
library(ROCR)
```

The goal of this lab is to optimize a Random Forest model using the same dataset from last week and then compare the results from the C5.0 method to the Random Forest method. The guidance this week is less prescriptive in terms of steps, so use the skills you have gained over the semester to build and evaluate the RF model and determine if the RF is a better approach or not. You will be graded on your model building, interpretation of the results and explanation of model selection. As always, rely on your teams but submit your own code. Lastly, there are likely several correct approaches involving a variety of different conclusions, just make sure your conclusions are supported by your approach.    

The dataset below includes Census data on 32,000+ individuals with a variety of variables and a target variable for above or below 50k in salary. 

Your goal is to build a Random Forest Classifier to be able to predict income levels above or below 50k. 

```{r}
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"

census <- read_csv(url, col_names = FALSE)

colnames(census) <- c("age","workclass","fnlwgt","education","education_num","marital_status","occupation","relationship","race","sex","capital_gain","capital_loss","hours_per_week","native_country","income")


View(census)
# ensures that the dataset does not contain any NAs
not <- census == "?"
is.na(census) <- not

census <- census[complete.cases(census), ]
View(census)
# because there are no false inputs in the table, no rows have to be removed
```


Recode the target variable to set the above 50k to 1 and below to 0, should already be done. 
```{r}
census$income <- recode(census$income, "<=50K"=0, ">50K"=1)
```

Ensure that the variables are correctly classified (should already be done)
```{r}
str(census)
census$workclass <- fct_collapse(census$workclass, employed = c("Federal-gov", "Local-gov", "State-gov", "Private","Self-emp-inc", "Self-emp-not-inc"), Unpaid="Without-pay")
census$education <- fct_collapse(census$education, below_college = c("1st-4th", "5th-6th", "7th-8th", "Preschool","9th", "10th", "11th", "12th", "HS-grad"), some_college = c("Assoc-acdm", "Assoc-voc", "Bachelors", "Some-college", "Prof-school", "Masters", "Doctorate", "Prof-school"))
census$native_country<- fct_collapse(census$native_country, US = c("United-States", "Outlying-US(Guam-USVI-etc)"), Not_US = c("Trinadad&Tobago", "Ecuador", "Columbia", "Peru","Cambodia", "China", "Vietnam", "Laos", "Taiwan", "Thailand", "Hong", "Japan", "Philippines", "India", "South", "Iran", "England", "Germany", "Italy","Portugal", "Poland","Holand-Netherlands", "Yugoslavia", "Greece", "Ireland", "France", "Hungary", "Scotland", "Mexico", "Puerto-Rico", "Dominican-Republic", "Haiti", "Guatemala", "Cuba","Honduras", "Jamaica", "Nicaragua", "El-Salvador", "Canada"))
census$occupation <- fct_collapse(census$occupation, desk=c("Adm-clerical", "Exec-managerial", "Sales", "Prof-specialty", "Tech-support"), labor=c("Farming-fishing", "Handlers-cleaners", "Machine-op-inspct", "Priv-house-serv", "Craft-repair", "Transport-moving", "Armed-Forces", "Protective-serv", "Other-service") )
census$relationship <- fct_collapse(census$relationship, family=c("Husband", "Wife", "Own-child", "Other-relative"), not_family=c("Not-in-family","Unmarried"))
census$marital_status <- fct_collapse(census$marital_status, married=c("Married-AF-spouse", "Married-civ-spouse", "Married-spouse-absent"), once_married = c("Divorced", "Separated", "Widowed"), single="Never-married")
census$race <- fct_collapse(census$race, white="White", not_white=c("Amer-Indian-Eskimo", "Asian-Pac-Islander", "Black", "Other"))
```

Finish any other data prep (one-hot encode, reduce factor levels)
```{r}
factors <- c(2, 4, 6, 7, 8, 9, 10)
census[factors] <- lapply(census[factors], factor)
data <- c(1, 2, 4, 6, 7, 8, 9, 10, 11, 13, 15)
census <- census[-c(3)]
str(census)

```

To make the random forest computation more efficient, I thought that I would drop the factors like working class and race as those did not seem to be important based on last week's lab. 


Create test, tune and training sets 
```{r}
sample_rows = 1:nrow(census)

set.seed(1984)
test_rows = sample(sample_rows,
                   dim(census)[1]*.10, #start with 10% of our dataset, could do 20%
                   # but random forest does require more training data because of the 
                   # sampling so 90% might be a better approach with this small of a dataset
                   replace = FALSE)# We don't want duplicate samples

str(test_rows)

# Partition the data between training and test sets using the row numbers the
# sample() function selected.
census_train = census[-test_rows,]
census_test = census[test_rows,]

# Check the output.
str(census_train)
str(census_test)

```

Calculate the initial mtry level 
```{r}
## square root of the predictors 
mytry_tune <- function(x){
  xx <- dim(x)[2]-1
  sqrt(xx)
}
       
mytry_tune(census)

str(census_train)

```

Run the initial RF model with 500 trees 
```{r}
set.seed(2023)

census_RF = randomForest(as.factor(income)~.,          #<- Formula: response variable ~ predictors.
                            #   The period means 'use all other variables in the data'.
                            census_train,     #<- A data frame with the variables to be used.
                            #y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
                            #subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
                            #xtest = NULL,       #<- This is already defined in the formula by the ".".
                            #ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
                            ntree = 500,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
                            mtry = 4,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
                            replace = TRUE,      #<- Should sampled data points be replaced.
                            #classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population 
                            #strata = NULL,      #<- Not necessary for our purpose here.
                            sampsize = 100,      #<- Size of sample to draw each time.
                            nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
                            #maxnodes = NULL,    #<- Limits the number of maximum splits. 
                            importance = TRUE,   #<- Should importance of predictors be assessed?
                            #localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
                            #proximity = TRUE,    #<- Should a proximity measure between rows be calculated?
                            norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
                            do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
                            keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
                            keep.inbag = TRUE, 
                            max_depth=6)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees? 
#Look at the output of the random forest 
census_RF

# Confusion matrix
census_RF$confusion

# check the accuracy of the model
census_RF_acc = sum(census_RF$confusion[row(census_RF$confusion) == col(census_RF$confusion)]) / sum(census_RF$confusion)

census_RF_acc # The accuracy is 0.84

#### Random forest output ####

# View the percentage of trees that voted for each data point to be in each class.
View(as.data.frame(census_RF$votes))

# The "predicted" argument contains a vector of predictions for each 
# data point.
View(as.data.frame(census_RF$predicted))

## Random forest output
View(as.data.frame(census_RF$importance)) #all the metrics together,not scaled
# shows that age, education_num, and marital status are important variables for the random forest 

# The "inbag" argument shows you which data point is included in which trees.
str(as.data.frame(census_RF$inbag))
View(as.data.frame(census_RF$inbag))

inbag <- as.data.frame(census_RF$inbag)

dim(census_RF$inbag)

# The "err.rate" argument includes a list of the cumulative error rates
# for each tree, by class and in aggregate for data points not 
# included in the tree (OOB).
View(as.data.frame(census_RF$err.rate))

err.rate <- as.data.frame(census_RF$err.rate)

View(err.rate)

# The "oob.times" argument includes the number of times that each data point
# is not excluded from trees in the random forest.
View(as.data.frame(census_RF$oob.times))


#### Visualize random forest results ####

# Let's visualize the results of the random forest.
# Let's start by looking at how the error rate changes as we add more trees.
census_RF_error = data.frame(1:nrow(census_RF$err.rate),
                                census_RF$err.rate)



colnames(census_RF_error) = c("Number of Trees", "Out of the Bag",
                                 "Salary<=50k", "Salary>50k")

# Add another variable that measures the difference between the error rates, in
# some situations we would want to minimize this but need to use caution because
# it could be that the differences are small but that both errors are really high,
# just another point to track. 

census_RF_error$Diff <-census_RF_error$`Salary>50k`-census_RF_error$`Salary<=50k`

View(census_RF_error)


library(plotly)

rm(fig)
fig <- plot_ly(x=census_RF_error$`Number of Trees`, y=census_RF_error$Diff,name="Diff", type = 'scatter', mode = 'lines')
fig <- fig %>% add_trace(y=census_RF_error$`Out of the Box`, name="OOB_Er")
fig <- fig %>% add_trace(y=census_RF_error$`Salary<=50k`, name="Salary below or equal to 50k")
fig <- fig %>% add_trace(y=census_RF_error$`Salary>50k`, name="Salary above")

fig

```

The confusion matrix shows that the classification error rate for a false positive is 5.39% as opposed to the classification error rate for a false negative which is 46.95%. In the context of the question, it is better that the model has a lower false positive rate because we want to be able to accurately predict citizens with a salary higher than 50k. 

The accuracy of the model was calculated to be 0.84. 

Sidenote: The proximity was commented out due to another studnet saying that it would allow the tree to run faster, and my code was unable to run due to the proximity being set as true. 

From the error plot, it shows that the error rate for predicting a salary below or equal to 50k is lower than 0.1 whereas the error rate for predicting a salary above 50k seems to be at 0.47. The error in the difference is at around 0.41 which is not terrible.


### Tuning the Model 
Using the training and tune datasets, tune the model in consideration of the number of trees, the number of variables to sample and the sample size that optimize the model output. 
```{r}
View(census_RF_error)
set.seed(2022)	
census_RF_2 = randomForest(as.factor(income)~.,          #<- formula, response variable ~ predictors.
                              #   the period means 'use all other variables in the data'.
                              census_train,     #<- A data frame with variables to be used.
                              #y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
                              #subset = NULL,      #<- This is unneccessary because we're using all the rows in the training data set.
                              #xtest = NULL,       #<- This is already defined in the formula by the ".".
                              #ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
                              ntree = 400,          #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
                              mtry = 5,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
                              replace = TRUE,      #<- Should sampled data points be replaced.
                              #classwt = NULL,     #<- Priors of the classes. We will work through this later. 
                              #strata = NULL,      #<- Not necessary for our purpose here.
                              sampsize = 200,      #<- Size of sample to draw each time.
                              nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
                              #maxnodes = NULL,    #<- The "nodesize" argument limits the number of maximum splits. 
                              importance = TRUE,   #<- Should importance predictors be assessed?
                              #localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
                              proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
                              norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
                              do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
                              keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
                              keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees? 

# I changed the sample size to be 200, the mtry to be 5, and the number of trees to be 400 to see if there were any improvements. 
census_RF$confusion 
census_RF_2$confusion

# It seems like the second model had a higher false positive classification error rate and a lower false negative classification rate compared to the first model 

#### Visualizing the model ####

# We can visualize the impact of the variables on the accuracy of 
# the model with the varImpPlot() function in the "randomForest" package.

varImpPlot(census_RF_2,     #<- the randomForest model to use
           sort = TRUE,        #<- whether to sort variables by decreasing order of importance
           n.var = 10,        #<- number of variables to display
           main = "Important Factors for Identifying Workers with a salary above 50k",
           #cex = 2,           #<- size of characters or symbols
           bg = "white",       #<- background color for the plot
           color = "blue",     #<- color to use for the points and labels
           lcolor = "orange")  #<- color to use for the horizontal lines
# The plot shows that the capital gain and age were the most important factors, followed by marital status and education number. 

## the tuneRF() function
str(census_train)
set.seed(2)
# census_RF_mtry = tuneRF(census_train[ ,-14],  #<- data frame of predictor variables
#                            (census_train[ ,14]),              #<- response vector (variables), factors for classification and continuous variable for regression
#                            mtryStart = 4,                        #<- starting value of mtry, the default is the same as in the randomForest function
#                            ntreeTry = 500,                        #<- number of trees used at the tuning step, let's use the same number as we did for the random forest
#                            stepFactor = 2,                       #<- at each iteration, mtry is inflated (or deflated) by this value
#                            improve = 0.05,                       #<- the improvement in OOB error must be by this much for the search to continue
#                            trace = TRUE,                         #<- whether to print the progress of the search
#                            plot = TRUE,                          #<- whether to plot the OOB error as a function of mtry
#                            doBest = FALSE)                       #<- whether to create a random forest using the optimal mtry parameter

str(census_train$income)

View(census_train)

# Size of the Forest 
# If you want to look at the size of the trees in the random forest, 
# or how many nodes each tree has, you can use the treesize() function.
treesize(census_RF_2,    #<- the randomForest object to use
         terminal = FALSE)  #<- when TRUE, only the terminal nodes are counted, when FALSE, all nodes are counted

# You can use the treesize() function to create a histogram for a visual presentation.
hist(treesize(census_RF_2,
              terminal = TRUE), main="Tree Size")

# The ROCR curve shows that the random forest model isn't necessarily the most optimal as it's close to the line. 

View(as.data.frame(census_RF_2$votes))

census_RF_2_prediction = as.data.frame(as.numeric(as.character(census_RF_2$votes[ ,2])))
View(census_RF_2_prediction)

census_train$income <- as.factor(census_train$income)
census_train_actual = data.frame(census_train[ ,14])

View(census_train_actual)
census_RF_2_prediction <- as.data.frame(census_RF_2_prediction)

census_prediction_comparison = prediction(census_RF_2_prediction, census_train_actual) 

View(census_prediction_comparison)

census_pred_performance = performance(census_prediction_comparison, measure="tpr", x.measure="fpr")

View(census_pred_performance)

census_rates = data.frame(fp = census_prediction_comparison@fp,  #<- false positive classification.
                             tp = census_prediction_comparison@tp,  #<- true positive classification.
                             tn = census_prediction_comparison@tn,  #<- true negative classification.
                             fn = census_prediction_comparison@fn)  #<- false negative classification.

colnames(census_rates) = c("fp", "tp", "tn", "fn")
View(census_rates)

str(census_rates)
tpr = census_rates$tp/(census_rates$tp+census_rates$fn)
fpr = census_rates$fp/(census_rates$fp+census_rates$tn)

census_rates_comparison = data.frame(census_pred_performance@x.values,census_pred_performance@y.values, fpr, tpr)
colnames(census_rates_comparison) = c("x.values", "y.values", "fpr", "tpr")

View(census_rates_comparison)

# plot the results 
plot(fpr, tpr, col = "blue", type = "l")
grid(col = "black")
abline(a = 0, b = 1, lwd = 2, lty = 2, col ="gray")

# Calculating the AUC value 
census_auc_RF = performance(census_prediction_comparison, 
                               "auc")@y.values[[1]]
census_auc_RF 
# The AUC value was calculated to be 0.9 which is pretty good for a model 

# Add the AUC value to the ROC plot.
text(x = 0.5, 
     y = 0.5, 
     labels = paste0("AUC = ", 
                     round(census_auc_RF,
                           2)))
```

### Evaluating the model using Test Data 
Once a final model has been selected, evaluate the model using the test dataset
```{r}
# Generating predictions with the model 
census_predict = predict(census_RF_2,      #<- a randomForest model
                            census_test,      #<- the test data set to use
                            type = "response",   #<- what results to produce, see the help menu for the options
                            predict.all = TRUE,  #<- should the predictions of all trees be kept?
                            proximity = FALSE)    #<- should proximity measures be computed

View(census_predict)

# Looking at the accuracy of the predictions 
View(census_predict$aggregate)
View(census_predict$individual)

## Error rate on the test set 
census_test_pred = data.frame(census_test,Prediction = census_predict$aggregate)
View(census_test_pred)

# ROC curve
census_roc <- roc(census_test_pred$income, as.numeric(census_test_pred$Prediction), plot = TRUE)

# Create the confusion matrix 
above_test_matrix_RF = table(census_test_pred$income, census_test_pred$Prediction)

above_test_matrix_RF 
# It seems like the model is better at predicting people with a salary below or equal to 50k 

# Calculate the misclassification rate 
above_test_error_rate_RF = sum(above_test_matrix_RF[row(above_test_matrix_RF) != col(above_test_matrix_RF)]) / sum(above_test_matrix_RF)

above_test_error_rate_RF 
# the misclassification rate is 0.15 which isn't bad and is good at training accuracy at 0.85

## Error rate on the test set 
# False Positive Rate (FP/(FP+TN))
132/(132+432) # 0.23

# Specificity 
1-0.2340426 # 0.77

# True Positive Rate and Sensitivity (TP/(TP+FN))
2139/(2139+313) # 0.87 which is really good 


library(caret)
confusionMatrix(as.factor(census_test_pred$Prediction),as.factor(census_test_pred$income),positive = "1", dnn=c("Prediction", "Actual"), mode = "everything")

# Evaluation Metrics
# The Kappa value is 0.56 which is a moderate level of agreement. The specificity was 0.9419 which is a good value and the Sensitivity was at a moderate level of 0.58. The F1 score was 0.66 which is considered moderate and the balanced accuracy was 0.76 which is good for this since the data set was unbalanced. 
```


Summarize your findings as compared to the C5.0 model from last week. Think about the time the model took to train, the model evaluation output and if the patterns generally between the two models are the same or different. What did you learn about the models or the data along the way? 

The findings from the random forest are about the same as the c5.0 model. However, a difference from the C5.0 model is that the random forest can take a lot of time depending on the variables that are set in the randomForest command. The evaluation metrics from the test data were calculated using the confusion matrix command. The Kappa value is 0.56 which is a moderate level of agreement. The specificity was 0.94 which is a good value and the Sensitivity was at a moderate level of 0.56. The F1 score was 0.65 which is considered moderate and the balanced accuracy was 0.75 which is good for this since the data set was unbalanced. The balanced accuracy for the c5.0 model was calculated to be 0.76 which is similar to that of the RandomForest. The AUC value for the random forest was better than that of the c5.0 model. The AUC value for the optimized random forest was 0.90 whereas the c5.0 model was around 0.82.

The patterns between the two models are generally the same, where the models are more accurate in determining someone with a salary below or equal to 50k as opposed to predicting a worker with a salary above 50k.The evaluation metrics are similar, but it seems like the c5.0 model had better statistics for the evaluation metric, but overall the c5.0 model and random forest model were decent at predicting a citizen's salary based on the variables given.  

I learned that the data was unbalanced and that some variables were more important such as age and capital gain which is similar to what the c5.0 model also found. The random forest model is interesting because it uses multiple trees and depending on the parameters that you set, such as the tree size or the sample size, does not always make a huge difference, but can slightly optimize a random forest model.

In terms of predicting a salary above or below 50k, I would say that this model would be good as a first iteration of the final product. The model could use more tweaking but the first impression of the model is that it can predict salaries below or equal to 50k well. 


### Changing Model Parameters
```{r}
set.seed(2022)	
census_RF_3 = randomForest(as.factor(income)~.,          #<- formula, response variable ~ predictors.
                              #   the period means 'use all other variables in the data'.
                              census_train,     #<- A data frame with variables to be used.
                              #y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
                              #subset = NULL,      #<- This is unneccessary because we're using all the rows in the training data set.
                              #xtest = NULL,       #<- This is already defined in the formula by the ".".
                              #ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
                              ntree = 600,          #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
                              mtry = 4,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
                              replace = TRUE,      #<- Should sampled data points be replaced.
                              #classwt = NULL,     #<- Priors of the classes. We will work through this later. 
                              #strata = NULL,      #<- Not necessary for our purpose here.
                              sampsize = 100,      #<- Size of sample to draw each time.
                              nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
                              #maxnodes = NULL,    #<- The "nodesize" argument limits the number of maximum splits. 
                              importance = TRUE,   #<- Should importance predictors be assessed?
                              #localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
                              proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
                              norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
                              do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
                              keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
                              keep.inbag = TRUE)  
census_RF_3$confusion
```


I thought that changing the mtry value might improve the RF model but the optimal mtry value was calculated to be 4. I thought it would also be interesting to create 100 trees as opposed to 500 trees to see how the model changed. The classification errors got worse, so I think that more trees are necessary. Other than those changes, all of the other parameters were kept the same. The classification error for a false negative got significantly higher and the classification error for a false positive was lower than the previous model. 
